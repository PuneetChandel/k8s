# POD

apiVersion: v1
kind: Pod
metadata:
 name: mypod
 labels:
  type: frontend
  app: myapp
 annotations:
   buildversoion: 1.1
  
spec: 

 nodeName: kube-01 # OR use nodeSelector or Nodeaffinity
 nodeSelector: 
    size: large #large is the label on Node $kk label nodes node01 size=large
 
 affinity: 
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
                
 tolerations: 
    - key: "spray"
      operator: "Equals"
      value: "mortein"
      effect: "NoSchedule"
   
 containers:
  - command: # in container
     - /bin/sh
     - -c
     - sleep 500
     - echo hello
    name: redis
    image: redis
    
    readinessProbe: # check if the app inside container is ready to accept requests
      tcpSocket: # httpGet, path, port | exec, command 
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
      
    livenessProbe: # check if the app inside container is healthy and not stuck/freeze and need to be restarted
      httpGet:
        path: /healthz
        port: 8080
      initialDelaySeconds: 15 # wait before test
      periodSeconds: 20 # how oftne to perform test
      failureThreshold: 3 # Number of failures before restart
      
    
    resources: # in container
      requests:
         memory: "1Gi"
         cpu: 1
      limits:
         memory: "2Gi" # terminate
         cpu: 2 # throttle
         
    securityContext: # in container
      runAsUser: 1000
      capabilities: ------------------> Only at Container Level not at POD
        add: ["NET_ADMIN", "SYS_TIME"] 
    
    ports:
        - containerPort: 80
        
    volumeMounts: # in container
    - mountPath: /test-ebs
      name: test-volume
    
    env: # in container
     - name: APP_USER
       value: myuser 
       
     - name : APP_COLOR
       valueFrom:
         confifMapKeyRef:
           name: app-config
           key: app_color
     
     - name : DB_pwd
       valueFrom:
         secretKeyRef:
           name: app-secret
           key: db_pass
           
    envFrom: # in container
      - configMapRef:
          name: app-config
      - secretRef:
           name: app-config

 volumes: # at Containers level
   - name: test-volume
     emptyDir: {}
   - name: config-vol
     configMap:
       name: app-config
   - name: secret-vol
     secret:
       secretName: app-secret
   - name: host-vol
     hostPath:
       path: /data
       type: Directory
       
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim


# ReplicaSet
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: myRS
 labels:
  type: rs
  app: myapp
spec:
 replicas: 3
 selector:
  matchLabels: # which POD to use
    type: frontend
 template:
  metadata: # from pod
  spec: # from pod
   containers: 
  

    
 # Deployment -- create RS as well

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginx-depl
   labels:
     type: depapp
     app: nginx-app
 
spec: 
  replicas: 3
  selector:
   matchLabels: # which POD to use
    type: frontend
    
  template:
   metadata: # from pod
   spec: # from pod
     containers: 
     
 
# Job , will create POD as well
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  backoffLimit: 10 --> number of retries before assuming the job has failed
  completions: 3 --> Number of completions, multiple Pods will be crated 
  parallelism: 3 --> Number of pararllel pods to spin until total completions are met
  template:
    spec: # Spec for Pod  
      containers: 
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
      

#Cron Job

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  
  schedule: "*/1 * * * *"
  
  jobTemplate:
    spec: # spec from Job
      completions: 3
      template:
        spec: # spec for Pod
          containers: 
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
          
   
# service
# Cluster IP - accessing Pods from Pods to avoid point-point connection 
# NodePort - accessing a frontend pod from different server

apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  type: NodePort | LoadBalancer | CluserIP
  ports:
    - targetPort: 9376 # by default the `targetPort` is set to the same value as the `port` field.
      port: 80 # Only MANDATORY Field 
      nodePort: 30007 # By default Kubernetes will allocate a port from a range (default: 30000-32767)
  
  selector: # Link Pod. Will link pods on multiple Nodes as well, and service can be accessed using any Nodes IP in that case
    app: myApp
    type: front-end
  
  
  # Ingress
  # first need a ingress controller like nginx, haproxy, GCP LB etc 
    ## LB, Monitor,ssl etc . Components for nginx-ingress-controller: Deployment, NodePort Service, ConfigMap, ServiceAccount
  # and then add a Ingress resources
  # Ingress
  
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: simple-fanout-example
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules: # multiple rules may be for multiple host/domains 
  - host: foo.bar.com 
    http:
      paths: # multiple paths for a host
      - path: /foo
        backend:
          serviceName: service1
          servicePort: 4200
      - path: /bar
        backend:
          serviceName: service2
          servicePort: 8080
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
          
  - http: ## DEFAULT without a hostname, [Also c
      paths:
      - backend:
          serviceName: service3
          servicePort: 80
          

## Single service Ingress
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  backend:
    serviceName: testsvc
    servicePort: 80
    
    
# Network Policy, Need network solutions on k8s like kube-router, Romana etc

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
spec:
  podSelector: # Pod to apply Policy On
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    ports:
    - protocol: TCP
      port: 5978
 

# Persistent Vol
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-volume
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain/Recycle/Delete # what happens when Claim on this PV is deleted
  hostPath:
     path: /tmp/data
 
# PVC -- binds to to PV, 1-1 mapping, PVC remains Pending if there is no matching PV

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce 
  resources:
    requests:
      storage: 8Gi
  
  selector: ## In case there are multiple PV with match capacity, Access, Volume Mode, storage class
    matchLabels:
      release: "stable"
   
      

      
